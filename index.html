<!DOCTYPE html>
<html>
    <head>
        <title>Deep Learning Final Project: Classifier to Segmentation</title>

        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta charset='UTF-8'>
        <meta name='color-scheme' content='dark light'>

        <link rel='stylesheet' type="text/css" href='assets/stylesheets/build/main.css'>
        <link rel='stylesheet' type="text/css" href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css'>

        <!-- Lato font -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300&display=swap" rel="stylesheet">

        <!-- Import AOS Library for card transitions -->
        <link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />

        <!-- jQuery -->
        <!--<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>-->
    </head>
    <body>
        <div class='profile'>
            <div class='profile-picture-container '>
                <img src='assets/images/pfp.png' class='profile-picture disable-select disable-drag' alt='Profile Picture' oncontextmenu="return false">
            </div>
            <div class='profile-description'>
                <h1 class='title-large'>Classifier to Segmentation</h1>
                <p class='text-medium'>Deep Learning Final Project by Abhay Deshpande, Andrew Mazzawi, Alan Wu, and Cindy Zou</p>
                <div class="icons">
                    <a target="_blank" href="https://youtu.be/i3IajV4LmRE" class='center'><div class='icon center'><i class="fab fa-brands fa-youtube"></i></div></a>
                    <a target="_blank" href="https://colab.research.google.com/drive/1NrTuwg91mDsVwlh8lbRf5tkdAejuXYbA?usp=sharing" class='center'><div class='icon center'><i class="fas fa-solid fa-code"></i></div></a>
                    <a target="_blank" href="https://github.com/Alan5032/Deep-Learning-Final-Project" class='center'><div class='icon center'><i class="fab fa-github fa-2x"></i></div></a>
                    <a target="_blank" href="mailto:abhayd@uw.edu,czou@uw.edu,amazza@cs.washington.edu,alan8621@uw.edu" class='center'><div class='icon center'><i class="fas fa-envelope fa-2x"></i></div></a>
                </div>
            </div>
            <div class='down-arrow center'>
                <a href='#experience' onclick='select("Experience")'>
                    <div class='arrow'></div>
                </a>
            </div>
        </div>

        <div class='portfolio-menu flex-vertical' data-aos='fade-down' data-aos-anchor='#experience' data-aos-anchor-placement='top-top' data-aos-offset='-60'>
            <div class='menu-buttons flex-horizontal'>
                <div class='name-container'>
                    <a href='#'>
                        <p class='text-small'>Classifier to Segmentation</p>
                    </a>
                </div>
                <button type="button" id='Experience' class='menu selected' onclick='select("Experience")'><a href='#experience'><h2 class='title-xs'>Introduction</h2></a></button>
                <button type="button" id='Coursework' class='menu' onclick='select("Coursework")'><a href='#coursework'><h2 class='title-xs'>Gallery</h2></a></button>
                <button type="button" id='Resume' class='menu' onclick='select("Resume")'><a href='#resume'><h2 class='title-xs'>Other</h2></a></button>
            </div>
        </div>

        <div class='portfolio flex-vertical'>
            <div id='experience'>
                <div class='title-container center'>
                    <h2 class='title-medium portfolio-title'>Problem Statement</h2>
                </div>
                <div class='section-container flex-vertical'>
                    <div class='experience card flex-horizontal' data-aos='fade'>
                        <div class='experience content flex-vertical'>
                            <div class='experience description'>
                                <p class='text-small'>
                                    We aim to solve the problem of object segmentation, where the input is an image and
                                    class label such as “cat” and the output is a mask where objects of that class are
                                    present in the image. However, we want to only use a pre-trained classification
                                    model with no additional training for segmentation, by checking which pixels of the
                                    image were used by the model to classify it as a particular label.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class='title-container center'>
                    <h2 class='title-medium portfolio-title'>Abstract</h2>
                </div>
                <div class='section-container flex-vertical'>
                    <div class='experience card flex-horizontal' data-aos='fade'>
                        <div class='experience content flex-vertical'>
                            <div class='experience description'>
                                <p class='text-small'>
                                    Current state of the art object segmentation methods involve training a deep neural
                                    network on a segmentation dataset. DeconvNet in particular appends deconvolutional
                                    layers after a convolutional neural network that aim to reverse the convolutions
                                    to obtain the input pixels that contributed to a class. However, Simonyan, et. al.
                                    in their paper “Deep inside Convolutional Networks: Visualising Image Classification
                                    Models and Saliency Maps” proposed a method for doing this same computation with only a
                                    pre-trained classification model and no additional training for segmentation. They
                                    additionally proved that their method is equivalent to deconvolutions except when
                                    dealing with a ReLU layer. They use the gradient of a class probability with respect
                                    to an input pixel as a proxy for how important the pixel is for identifying that class,
                                    creating a saliency map, which can then be converted into a segmentation mask.
                                    A second method, guided backpropagation, tries to improve upon Simonyan, et. al.’s work.
                                    We replicate Simonyan, et. al.’s method and compare it in detail to guided backpropagation
                                    on the COCO segmentation challenge. We were able to achieve high IOU compared to ground
                                    truth masks on images with one large object, but lower IOU on images with many or small objects.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class='title-container center'>
                    <h2 class='title-medium portfolio-title'>Methodology</h2>
                </div>
                <div class='section-container flex-vertical'>
                    <div class='experience card flex-horizontal' data-aos='fade'>
                        <div class='experience content flex-vertical'>
                            <div class='experience description'>
                                <p><strong>Pipeline Summary</strong></p>
                                <p class='text-small'>
                                    1. Get detected classes with TResNet (multi-class classifier trained on COCO. Input: image. Output: probabilities for each class)
                                </p>
                                <p class='text-small'>
                                    &emsp; a. Run the classification model forward on image to get class probabilities
                                </p>
                                <p class='text-small'>
                                    &emsp; b. Get top k=20 most likely classes
                                </p>
                                <p class='text-small'>
                                    &emsp; c. Filter for classes with probability > threshold=0.75
                                </p>
                                <div class='inline-image'>
                                    <img src='assets/images/methodology1.png' class='project image'>
                                </div>
                                <p class='text-small'>
                                    2. For each detected class:
                                </p>
                                <p class='text-small'>
                                    &emsp; a. Calculate the gradient of the class probability w.r.t. the input image
                                </p>
                                <p class='text-small'>
                                    &emsp; &emsp; i. Method 1: “vanilla” gradient backpropagation
                                </p>
                                <div class='inline-image'>
                                    <img src='assets/images/methodology2.png' class='project image'>
                                </div>
                                <p class='text-small'>
                                    &emsp; &emsp; ii. Method 2: guided backpropagation
                                </p>
                                <div class='inline-image'>
                                    <img src='assets/images/methodology3.png' class='project image'>
                                </div>
                                <p><strong>"Vanilla" Gradient Backpropagation</strong></p>
                                <p class='text-small'>
                                    This method is called “vanilla” because the idea behind it is simple.
                                    Simonyan, et. al. proposed that if the gradient of a class’s probability with
                                    regard to an input pixel is large, then that means changing the pixel would cause a
                                    large change in the class’ probability, so it must be important for identifying that class.
                                    Thus the absolute value of the gradient of a class’s probability with regard to all the
                                    pixels in an image is a saliency map for that class. We can get this gradient easily using
                                    PyTorch Autograd by calling backward() on the model output, which populates the gradient of
                                    the output with regard to each layer, and then getting the gradient of the input image.
                                </p>
                                <p><strong>Guided Backpropagation</strong></p>
                                <p class='text-small'>
                                    Guided backpropagation aims to solve some of the shortcomings of the vanilla method,
                                    where the saliency map is fuzzy and pixels that negatively correlate with the class
                                    still have high saliency. We backpropagate the same way, except for a ReLU layer we
                                    zero out gradient entries that are either negative or whose pixel was zeroed out in
                                    the ReLU forward pass. Intuitively we want to zero out negative gradient pixels because
                                    they negatively correlate with the output class. Simonyan, et. al. showed that vanilla
                                    gradient backpropagation is equivalent to deconvolutions, except when backpropagating
                                    through a ReLU layer. Guided backpropagation’s treatment of ReLU layers is closer to
                                    that of deconvolutions.
                                </p>
                                <div class='inline-image'>
                                    <img src='assets/images/methodology14.png' class='project image'>
                                </div>
                                <div class='inline-image'>
                                    <img src='assets/images/methodology15.png' class='project image'>
                                </div>
                                <div class='inline-image'>
                                    <img src='assets/images/methodology16.png' class='project image'>
                                </div>
                                <p><strong>Gradient to Saliency Map</strong></p>
                                <p class='text-small'>
                                    We used the method proposed by Simonyan et. al., which is to take the absolute value
                                    of the gradient and flatten the channels by taking the max absolute value across
                                    channels for each pixel. The absolute value is the magnitude of the gradient, or,
                                    how important it is for the classification. We also explored not taking the absolute
                                    value, but this resulted in masks with holes, as shown in the figures below. The
                                    figures contain, from left to right: input image, saliency map, mask, masked image.
                                    Detected class: “bird.”
                                </p>
                                <div class='inline-image'>
                                    <img src='assets/images/methodology17.png' class='project image'>
                                </div>
                                <div class='inline-image'>
                                    <img src='assets/images/methodology18.png' class='project image'>
                                </div>
                                <p><strong>Saliency Map to Segmentation Mask</strong></p>
                                <p class='text-small'>
                                    Using the saliency map of the given image, the program extracts pixels with an
                                    intensity that is at the bottom 30% to and pixels at or above the top 90% to create
                                    a background and foreground image respectively. Then by using the foreground image,
                                    we iterate through the top 50 pixels with the highest saliency and recursively expand
                                    outwards to pixels above a certain threshold in an attempt to create a mask.
                                    This approach generated an inaccurate mask for some images and for others nothing
                                    was generated. To fix this, we used a library that implemented the Graph Cut
                                    algorithm to segment the image. This implementation of graph cut works by taking
                                    the strokes of the seed image that mark foreground and background and perform a cut
                                    on the image. The cut is performed by choosing the cut with the least cumulative weight,
                                    where weight of an edge is based on the inverse squared difference between neighboring
                                    pixels, such that pixels (pixels in the graph are represented by the nodes) with
                                    large differences have low weight and vice versa. After the cuts are created, the
                                    mask is output separating the foreground and background.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class='title-container center'>
                    <h2 class='title-medium portfolio-title'>Evaluation</h2>
                </div>
                <div class='section-container flex-vertical'>
                    <div class='experience card flex-horizontal' data-aos='fade'>
                        <div class='experience content flex-vertical'>
                            <div class='experience description'>
                                <p class='text-small'>
                                    For evaluation, we combined the segmentation masks for all detected classes within an
                                    image into a single mask for that image. Then, we compared our generated mask for an
                                    image to its ground truth mask. We calculated how well our generated masks were by
                                    calculating the Intersection over Union value between the generated masks and their
                                    respective ground truth masks. An Intersection over Union value is the amount of
                                    intersecting area between the two masks divided by the union of the two masks’ area.
                                    Typically, having an IoU value greater than 0.5 is considered good.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div id='coursework'>
                <div class='title-container center'>
                    <h2 class='title-medium portfolio-title'>Examples</h2>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project title'>
                            <p class='title-small'>Raw Image</p>
                        </div>
                        <div class='project description'>
                            <p class='text-small'>Raw images are taken from the COCO dataset and transformed to be 448 pixels wide and tall.</p>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project title'>
                            <p class='title-small'>Truth Mask</p>
                        </div>
                        <div class='project description'>
                            <p class='text-small'>These images are the raw images overlaid with the ground truth segmentation masks.</p>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project title'>
                            <p class='title-small'>Generated Mask</p>
                        </div>
                        <div class='project description'>
                            <p class='text-small'>These images are the raw images overlaid with the segmentation masks that we generate. These images are labeled with their Intersection over Union (IoU) values, which are calculated in comparison to their respective ground truth masks.</p>
                        </div>
                    </div>
                </div>

                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image0.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask0.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image0.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.727</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image1.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask1.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image1.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.252</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image2.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask2.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image2.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.466</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image3.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask3.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image3.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.168</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image4.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask4.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image4.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.348</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image5.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask5.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image5.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.256</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image6.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask6.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image6.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.358</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image7.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask7.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image7.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.178</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image8.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask8.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image8.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.543</figcaption>
                    </div>
                </div>
                <div class='section-container grid-3'>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/val_image9.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/truth_mask9.png' class='project image'>
                        </div>
                    </div>
                    <div class='project card flex-vertical' data-aos='fade'>
                        <div class='project image-container center'>
                            <img src='assets/images/mask_image9.png' class='project image'>
                        </div>
                        <figcaption>IoU : 0.632</figcaption>
                    </div>
                </div>
            </div>

            <div id='projects'>
                <div class='title-container center'>
                    <h2 class='title-medium portfolio-title'>Results</h2>
                </div>
                <div class='section-container flex-vertical'>
                    <div class='experience card' data-aos='fade'>
                        <div class='experience description'>
                            <p class='text-small'>
                                As shown in the evaluation images, we get mixed results for the IOU. Images with a single
                                large object generally have high IOU, while images with many different objects or small
                                objects tend to have lower IOU. We could have made our masks more precise by running the
                                GraphCut algorithm on multiple scaled versions of the image to better capture small objects.
                                We could also use some techniques to reduce noise.
                            </p>
                            <p class='text-small'>
                                Sometimes saliency is highly concentrated on one area of an object, such as a person’s face or the tip of a fork
                            </p>
                            <div class='inline-image'>
                                <img src='assets/images/result1.png' class='project image'>
                            </div>
                            <div class='inline-image'>
                                <img src='assets/images/result2.png' class='project image'>
                            </div>
                            <p class='text-small'>
                                Guided backpropagation seems to do about the same or worse compared to vanilla gradient
                                with our particular method of computing masks. This could be because zeroing out negatives
                                in the backpropagation loses important information. It could also be that our method of
                                computing the mask isn’t suited for guided backpropagation, since it tends to highlight
                                edges. Other papers have instead used an edge detection algorithm to get the outlines of objects.
                            </p>
                            <div class='inline-image'>
                                <img src='assets/images/result3.png' class='project image'>
                            </div>
                            <div class='inline-image'>
                                <img src='assets/images/result4.png' class='project image'>
                            </div>
                        </div>
                    </div>
                </div>
                <div class='title-container center'>
                    <h2 class='title-medium portfolio-title'>Related Work</h2>
                </div>
                <div class='section-container flex-vertical'>
                    <div class='experience card' data-aos='fade'>
                        <div class='experience description'>
                            <p><strong>Dataset</strong></p>
                            <p class='text-small'>
                                Tsung-Yi Lin, Maire, M., Belongie, S. J., Bourdev, L. D., Girshick, R. B., Hays, J., … Zitnick, C. L. (2014).
                                Microsoft COCO: Common Objects in Context. CoRR, abs/1405.0312. Retrieved from
                                http://arxiv.org/abs/1405.0312
                            </p>
                            <a href="https://cocodataset.org/#download">2014 Val images and annotations</a>
                            <br>

                            <p><strong>Classifier</strong></p>
                            <a href="https://github.com/Alibaba-MIIL/ML_Decoder">https://github.com/Alibaba-MIIL/ML_Decoder</a>
                            <br>

                            <p><strong>References</strong></p>
                            <p class='text-small'>
                                Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. “Deep inside Convolutional
                                Networks: Visualising Image Classification Models and Saliency Maps.” arXiv.org,
                                April 19, 2014. https://arxiv.org/abs/1312.6034.
                            </p>
                            <a href="https://arxiv.org/pdf/1312.6034.pdf">https://arxiv.org/pdf/1312.6034.pdf</a>
                            <p class='text-small'>
                                Zeiler, Matthew D., and Rob Fergus. “Visualizing and Understanding Convolutional Networks.”
                                ArXiv.org, 28 Nov. 2013, arxiv.org/abs/1311.2901. Accessed 14 Dec. 2022.
                            </p>
                            <a href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</a>
                            <p class='text-small'>
                                Zabriskie, Nathan. “Nathanzabriskie/Graphcut: Graph Cut Image Segmentation with Custom GUI.”
                                GitHub, October 1, 2017. https://github.com/NathanZabriskie/GraphCut.
                            </p>
                            <a href="https://github.com/NathanZabriskie/GraphCut">https://github.com/NathanZabriskie/GraphCut</a>
                            <p class='text-small'>
                                Guerzhoy, Michael. “CSC321: Introduction to Machine Learning and Neural Networks (Winter 2016).”
                                CSC321: Introduction to machine learning and neural networks (winter 2016). Accessed December 14, 2022.
                                https://www.cs.toronto.edu/~guerzhoy/321/.
                            </p>
                            <a href="https://www.cs.toronto.edu/~guerzhoy/321/lec/W07/HowConvNetsSee.pdf">
                                https://www.cs.toronto.edu/~guerzhoy/321/lec/W07/HowConvNetsSee.pdf
                            </a>
                            <p class='text-small'>
                                Draelos, Rachel. “CNN Heat Maps: Saliency/Backpropagation.” Glass Box, 21 June 2019,
                                glassboxmedicine.com/2019/06/21/cnn-heat-maps-saliency-backpropagation/.
                                Accessed 14 Dec. 2022.
                            </p>
                            <a href="https://glassboxmedicine.com/2019/06/21/cnn-heat-maps-saliency-backpropagation/">
                                https://glassboxmedicine.com/2019/06/21/cnn-heat-maps-saliency-backpropagation/
                            </a>
                            <p class='text-small'>
                                Draelos, Rachel. “CNN Heat Maps: Gradients vs. DeconvNets vs. Guided Backpropagation.”
                                Glass Box, 6 Oct. 2019, glassboxmedicine.com/2019/10/06/cnn-heat-maps-gradients-vs-deconvnets-vs-guided-backpropagation/.
                                Accessed 14 Dec. 2022.
                            </p>
                            <a href="https://glassboxmedicine.com/2019/10/06/cnn-heat-maps-gradients-vs-deconvnets-vs-guided-backpropagation/">
                                https://glassboxmedicine.com/2019/10/06/cnn-heat-maps-gradients-vs-deconvnets-vs-guided-backpropagation/
                            </a>
                        </div>
                    </div>
                </div>
            </div>
            
            <div id='resume'>
                <div class='title-container center'>
                    <h2 class='title-medium portfolio-title'>Other</h2>
                </div>
                <div class="section-container flex-vertical">
                    <div class='resume card'>
                        <a target='_blank' href="https://colab.research.google.com/drive/1NrTuwg91mDsVwlh8lbRf5tkdAejuXYbA?usp=sharing" class="full plain">
                            <h2 class='text-medium'>View Google Colab Notebook</h2>
                        </a>
                    </div>
                    <div class='resume card'>
                        <a target='_blank' href="https://youtu.be/i3IajV4LmRE" class="full plain">
                            <h2 class='text-medium'>Watch Demo Video</h2>
                        </a>
                    </div>
                </div>
            </div>
        </div>

        <div class='dark-mode-slider center' onclick='toggleDarkMode()' data-theme='Light Theme'>
            <div class='dark-mode-thumb center'><i class='fas fa-sun'></i></div>
        </div>

        <footer>
            <p class='text-small'>Website theme from <a href='https://arnavthareja.github.io' target='_blank' class='plain'>Arnav Thareja</a></p>
        </footer>

        <!-- Link scripts -->
        <script src='assets/scripts/index.js'></script>

        <!-- Initialize AOS Library -->
        <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
        <script>
            AOS.init();
        </script>
    </body>
</html>
